# -*- coding: utf-8 -*-
"""Практическое применение Алгоритмов ML.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yaP3rygwcXvSxy1XVTp2TSGjOK65BigS
"""



"""# Практическое применение Алгоритмов ML

## Что такое алгоритмы машинного обучения

Алгоритмы машинного обучения — это набор методов, которые позволяют компьютерам учиться на данных и делать прогнозы или классификации без явного программирования. Эти алгоритмы делятся на несколько типов:

## Типы ML-алгоритмов

1. **Обучение с учителем (Supervised Learning)**: Используется для задач классификации и регрессии, где алгоритм обучается на помеченных данных.
2. **Обучение без учителя (Unsupervised Learning)**: Используется для кластеризации и поиска закономерностей в данных без предварительной маркировки.
3. **Обучение с подкреплением (Reinforcement Learning)**: Используется для обучения агентов принимать решения на основе наград или штрафов.

*Чтобы познакомиться с алгоритмами или углубиться в тему — предлагаем ознакомиться с нашей статьей об алгоритмах: https://vk.cc/cJHYXB.*


## Линейная регрессия (Linear Regression)

Линейная регрессия — это метод прогнозирования, который моделирует линейную зависимость между переменными. Он используется для предсказания значения целевой переменной на основе одной или нескольких независимых переменных.

**Пример кода**: прогнозирование стоимости недвижимости в зависимости от размера и расположения объекта.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Генерируем данные
np.random.seed(0)  # Для воспроизводимости результатов

# Площадь (м²)
area = np.random.uniform(50, 200, size=100)

# Местоположение (рейтинг от 1 до 10)
location = np.random.uniform(1, 10, size=100)

# Цена (тыс. руб.) с добавлением случайной ошибки
price = 100 + 2 * area + 10 * location + np.random.normal(0, 50, size=100)

# Создаем матрицу признаков
X = np.column_stack((area, location))

# Создаем модель линейной регрессии
model = LinearRegression()

# Обучаем модель
model.fit(X, price)

# Получаем коэффициенты модели
print("Коэффициенты модели: ", model.coef_)
print("Константа модели: ", model.intercept_)

# Визуализация данных и линии регрессии
plt.figure(figsize=(10, 6))

# Визуализация данных
plt.scatter(area, price, label='Данные')

# Построение линии регрессии для площади
area_pred = np.linspace(50, 200, 100)
location_pred = np.full(100, np.mean(location))  # Среднее местоположение
price_pred = model.predict(np.column_stack((area_pred, location_pred)))
plt.plot(area_pred, price_pred, label='Линия регрессии', color='red')

plt.xlabel('Площадь (м²)')
plt.ylabel('Цена (тыс. руб.)')
plt.title('Прогнозирование цен на недвижимость')
plt.legend()
plt.show()

"""## Линейный дискриминантный анализ (LDA)

LDA — это метод, используемый для классификации объектов в группы на основе их признаков. Он помогает найти линейные комбинации признаков, которые лучше всего разделяют классы.

**Пример кода**:
"""

import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Данные
iris = load_iris()
X = iris.data
y = iris.target

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Модель LDA
lda = LDA(n_components=2)
lda.fit(X_train, y_train)

# Преобразование данных
X_train_lda = lda.transform(X_train)
X_test_lda = lda.transform(X_test)

# Визуализация результатов
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_train_lda[:, 0], X_train_lda[:, 1], c=y_train, cmap='viridis')
plt.title('Обучающая выборка после LDA')
plt.xlabel('Первый компонент')
plt.ylabel('Второй компонент')

plt.subplot(1, 2, 2)
plt.scatter(X_test_lda[:, 0], X_test_lda[:, 1], c=y_test, cmap='viridis')
plt.title('Тестовая выборка после LDA')
plt.xlabel('Первый компонент')
plt.ylabel('Второй компонент')

plt.tight_layout()
plt.show()

"""**Объяснение кода**: В этом примере мы используем LDA для уменьшения размерности данных набора iris до двух признаков. Это помогает визуализировать данные и улучшить классификацию.

## Логистическая регрессия (Logistic Regression)

Логистическая регрессия — это метод классификации, который используется для прогнозирования вероятности принадлежности объекта к одной из двух категорий.

**Пример кода**: классификация электронных писем как спам или не-спам на основе двух признаков: количества ссылок и количества слов в письме:
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# Генерируем данные
np.random.seed(0)  # Для воспроизводимости результатов

# Количество ссылок в письме
links = np.random.uniform(0, 10, size=100)

# Количество слов в письме
words = np.random.uniform(0, 100, size=100)

# Метка класса (0 - не-спам, 1 - спам)
# Примерно 20% писем будут спамом
spam_prob = 0.2
spam = np.random.rand(100) < spam_prob

# Добавляем шум в данные, чтобы сделать их более реалистичными
# Спам-сообщения обычно имеют больше ссылок и меньше слов
links[spam] += np.random.uniform(0, 5, size=np.sum(spam))
words[spam] -= np.random.uniform(0, 20, size=np.sum(spam))

# Создаем матрицу признаков
X = np.column_stack((links, words))

# Создаем модель логистической регрессии
model = LogisticRegression(max_iter=1000)

# Обучаем модель
model.fit(X, spam.astype(int))

# Получаем коэффициенты модели
print("Коэффициенты модели: ", model.coef_)
print("Константа модели: ", model.intercept_)

# Визуализация данных и границы классификации
plt.figure(figsize=(10, 6))

# Визуализация данных
plt.scatter(links[~spam], words[~spam], label='Не-спам', color='blue')
plt.scatter(links[spam], words[spam], label='Спам', color='red')

# Построение границы классификации
x_min, x_max = links.min(), links.max()
y_min, y_max = words.min(), words.max()
xx, yy = np.meshgrid(np.linspace(x_min, x_max), np.linspace(y_min, y_max))
Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)

plt.contour(xx, yy, Z, [0.5], colors='green', linestyles=['--'])

plt.xlabel('Количество ссылок')
plt.ylabel('Количество слов')
plt.title('Классификация писем как спам или не-спам')
plt.legend()
plt.show()

"""## Алгоритм построения дерева решений (Decision Tree)

Дерево решений — это метод классификации и регрессии, который использует дерево для разделения данных на основе признаков.

**Пример кода**: прогнозирование вероятности покупки продукта на основе демографических данных
"""

import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn import tree

# Генерируем данные
np.random.seed(0)  # Для воспроизводимости результатов

# Возраст
age = np.random.randint(18, 60, size=100)

# Доход
income = np.random.randint(50000, 150000, size=100)

# Метка класса (0 - не купил, 1 - купил)
buy_prob = 0.2
buy = np.random.rand(100) < buy_prob

# Добавляем шум в данные
age[buy] += np.random.randint(0, 5, size=np.sum(buy))
income[buy] += np.random.randint(0, 20000, size=np.sum(buy))

# Создаем матрицу признаков
X = np.column_stack((age, income))

# Создаем модель дерева решений с ограниченной глубиной
model = DecisionTreeClassifier(max_depth=3)  # Ограничиваем глубину до 2 уровней

# Разделяем данные на обучающие и тестовые
X_train, X_test, y_train, y_test = train_test_split(X, buy.astype(int), test_size=0.2, random_state=42)

# Обучаем модель
model.fit(X_train, y_train)

# Визуализация дерева
plt.figure(figsize=(20, 13))
tree.plot_tree(model, feature_names=['Возраст', 'Доход'], class_names=['Не купил', 'Купил'], filled=True)
plt.show()

"""## Метод К-ближайших соседей (K-Nearest Neighbors)

KNN — это метод классификации, который присваивает объекту тот же класс, что и большинству его ближайших соседей.

**Пример кода**:
"""

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Данные
iris = load_iris()
X = iris.data
y = iris.target

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Модель KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Классификация
y_pred = knn.predict(X_test)

# Уменьшение размерности с помощью PCA
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Визуализация
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train)
plt.title('Обучающие данные')

plt.subplot(1, 2, 2)
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_pred)
plt.title('Тестовые данные (предсказания)')

plt.show()

"""**Объяснение кода**: Этот пример показывает, как использовать KNN для классификации. Мы устанавливаем количество ближайших соседей (`n_neighbors=5`) и обучаем модель на данных.

## Метод K-средних (K-Means Clustering)

K-Means — это алгоритм кластеризации, который группирует объекты в кластеры на основе их сходства.

**Пример кода**:
"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np

# Данные
X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])

# Модель K-Means
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)

# Кластеры
labels = kmeans.labels_

# Визуализация
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=200, c='red')
plt.title('Кластеризация с помощью K-Means')
plt.show()

"""**Объяснение кода**: В этом примере мы используем K-Means для кластеризации данных. Мы устанавливаем количество кластеров (`n_clusters=2`) и обучаем модель на данных.

## Апостериорная кластеризация (Hierarchical Clustering)

Иерархическая кластеризация — это метод, который строит иерархию кластеров путем слияния или разделения кластеров.

**Пример кода**:
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Генерируем данные
np.random.seed(0)  # Для воспроизводимости результатов
data = np.random.rand(10, 2)  # 10 точек в 2D-пространстве

# Выполняем иерархическую кластеризацию
Z = linkage(data, 'ward')  # Используем алгоритм Ward для кластеризации

# Визуализируем дендрограмму
plt.figure(figsize=(10, 6))
dendrogram(Z, leaf_rotation=90., leaf_font_size=12.)
plt.title('Дендрограмма иерархической кластеризации')
plt.show()

# Применяем иерархическую кластеризацию с определенным количеством кластеров
cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
labels = cluster.fit_predict(data)

# Визуализируем результаты кластеризации
plt.figure(figsize=(10, 6))
plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')
plt.title('Результаты иерархической кластеризации')
plt.show()

"""**Объяснение кода**: Этот пример показывает, как использовать иерархическую кластеризацию для группировки данных. Мы устанавливаем количество кластеров (`n_clusters=3`) и обучаем модель на данных.

Также код отображает дендрограмму, которая показывает, как кластеры сливаются на каждом уровне.

## Сигмоида (Sigmoid Function)

Сигмоида — это математическая функция, часто используемая в логистической регрессии для преобразования линейных комбинаций в вероятности.

**Формула сигмоиды**:
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$

**Пример кода**:
"""

import numpy as np
import matplotlib.pyplot as plt

# Определение сигмоидной функции
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Генерация значений x
x = np.linspace(-10, 10, 100)

# Вычисление значений y
y = sigmoid(x)

# Построение графика
plt.plot(x, y)
plt.title('График сигмоидной функции')
plt.xlabel('x')
plt.ylabel('sigmoid(x)')
plt.grid(True)
plt.show()

"""**Объяснение кода**: Этот код сначала генерирует значения `x` в диапазоне от `-10` до `10`, затем вычисляет соответствующие значения `y` с помощью сигмоидной функции. Наконец, он строит график, отображая зависимость `y` от `x`. График сигмоиды имеет характерную форму, где значения функции приближаются к `0` при больших отрицательных `x` и к `1` при больших положительных `x`.

## Сети векторного квантования

Сети векторного квантования — это методы, используемые для сжатия данных путем представления их в виде векторов.

## Метод опорных векторов (Support Vector Machines)

SVM — это алгоритм классификации, который находит гиперплоскость, которая максимально разделяет классы в многомерном пространстве.

**Пример использования SVM для классификации данных:**:
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

# Генерируем данные
np.random.seed(0)  # Для воспроизводимости результатов

# Класс 1
class1_x = np.random.normal(0, 1, 50)
class1_y = np.random.normal(0, 1, 50)

# Класс 2
class2_x = np.random.normal(5, 1, 50)
class2_y = np.random.normal(5, 1, 50)

# Объединяем данные
X = np.column_stack((np.concatenate((class1_x, class2_x)), np.concatenate((class1_y, class2_y))))
y = np.concatenate((np.zeros(50), np.ones(50)))

# Создаем модель SVM
model = svm.SVC(kernel='linear')

# Обучаем модель
model.fit(X, y)

# Визуализация данных и гиперплоскости
plt.figure(figsize=(8, 6))
plt.scatter(class1_x, class1_y, label='Класс 1', color='blue')
plt.scatter(class2_x, class2_y, label='Класс 2', color='red')

# Визуализация гиперплоскости
w = model.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1)
yy = a * xx - (model.intercept_[0]) / w[1]
plt.plot(xx, yy, 'k-')

# Визуализация опорных векторов
plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none', edgecolors='black')

plt.title('Классификация с помощью SVM')
plt.xlabel('Признак 1')
plt.ylabel('Признак 2')
plt.legend()
plt.show()

"""**Объяснение кода**: В этом примере мы используем SVM для бинарной классификации. Мы выбираем линейное ядро (`kernel='linear'`) и обучаем модель на данных.

## Метод случайного леса (Random Forest)

Случайный лес — это ансамблевый метод, который объединяет множество деревьев решений для повышения точности классификации.

**Пример кода**:
"""

import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix

# Данные
iris = load_iris()
X = iris.data
y = iris.target

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Модель случайного леса
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

# Классификация
y_pred = rf.predict(X_test)

# Визуализация матрицы ошибок
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap='Blues')
plt.title('Матрица ошибок')
plt.colorbar()
plt.show()

# Уменьшение размерности с помощью PCA
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Визуализация данных
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train)
plt.title('Обучающие данные')

plt.subplot(1, 2, 2)
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_pred)
plt.title('Тестовые данные (предсказания)')

plt.show()

"""**Объяснение кода**: В этом примере мы используем случайный лес для классификации видов ириса. Мы устанавливаем количество деревьев (`n_estimators=100`) и обучаем модель на данных.

## Бэггинг

Бэггинг — это метод ансамблевого обучения, который использует несколько моделей, обученных на случайных подвыборках данных.

**Пример кода**:
"""

import matplotlib.pyplot as plt
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

# Данные
iris = load_iris()
X = iris.data
y = iris.target

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Модель бэггинга
bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)
bagging.fit(X_train, y_train)

# Классификация
y_pred = bagging.predict(X_test)

# Уменьшение размерности с помощью PCA
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Визуализация
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train)
plt.title('Обучающие данные')

plt.subplot(1, 2, 2)
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_pred)
plt.title('Тестовые данные (предсказания)')

plt.show()

"""**Объяснение кода**: Этот код демонстрирует, как использовать бэггинг с деревьями решений для классификации и визуализировать результаты с помощью PCA. Визуализация помогает оценить качество классификации и понять, как данные были сгруппированы.

## Чем различается случайный лес и бэггинг деревьев
Случайный лес — это специальный случай бэггинга, где используются деревья решений с дополнительной случайностью при выборе признаков.

## Бустинг

Бустинг — это метод, который последовательно обучает модели, фокусируясь на ошибках предыдущих моделей.

**Адаптивный бустинг (AdaBoost)**: Использует взвешенные голоса моделей для улучшения точности классификации.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Генерируем данные
np.random.seed(0)  # Для воспроизводимости результатов

# Класс 1
class1_x = np.random.normal(0, 1, 50)
class1_y = np.random.normal(0, 1, 50)

# Класс 2
class2_x = np.random.normal(5, 1, 50)
class2_y = np.random.normal(5, 1, 50)

# Объединяем данные
X = np.column_stack((np.concatenate((class1_x, class2_x)), np.concatenate((class1_y, class2_y))))
y = np.concatenate((np.zeros(50), np.ones(50)))

# Разделяем данные на обучающие и тестовые
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Создаем модель AdaBoost
model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=10, random_state=42)

# Обучаем модель
model.fit(X_train, y_train)

# Визуализация данных и границы классификации
plt.figure(figsize=(10, 6))
plt.scatter(class1_x, class1_y, label='Класс 1', color='blue')
plt.scatter(class2_x, class2_y, label='Класс 2', color='red')

x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max), np.linspace(y_min, y_max))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contour(xx, yy, Z, [0.5], colors='green', linestyles=['--'])

plt.title('Классификация с помощью AdaBoost')
plt.xlabel('Признак 1')
plt.ylabel('Признак 2')
plt.legend()
plt.show()

"""**Градиентный бустинг (Gradient Boost)**: Использует градиентный спуск для оптимизации ошибок предыдущих моделей."""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor

# Генерируем данные
np.random.seed(0)  # Для воспроизводимости результатов

# Входные данные (x)
x = np.linspace(-10, 10, 100)

# Выходные данные (y) с шумом
y = np.sin(x) + np.random.normal(0, 0.5, size=len(x))

# Создаем матрицу признаков
X = x.reshape(-1, 1)

# Создаем модель градиентного бустинга
model = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, random_state=42)

# Обучаем модель
model.fit(X, y)

# Визуализация прогресса модели
plt.figure(figsize=(10, 6))

# Визуализация исходных данных
plt.scatter(x, y, label='Исходные данные', color='blue', alpha=0.5)

# Визуализация прогресса модели на каждом шаге
for i in range(1, model.n_estimators + 1):
    model_partial = GradientBoostingRegressor(n_estimators=i, learning_rate=0.1, random_state=42)
    model_partial.fit(X, y)
    y_pred = model_partial.predict(X)
    plt.plot(x, y_pred, label=f'Шаг {i}')

plt.plot(x, model.predict(X), label='Финальный прогноз', color='red', linewidth=2)

plt.title('Прогресс модели градиентного бустинга')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

"""**XGBoost**: Модификация градиентного бустинга с улучшенной производительностью и скоростью обучения."""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor

# Генерация данных для регрессии
X, y = make_regression(n_samples=100, n_features=1, noise=0.1)

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Модель XGBoost
xgb_model = XGBRegressor(objective='reg:squarederror', max_depth=5, learning_rate=0.1, n_estimators=100)

# Обучение модели
xgb_model.fit(X_train, y_train)

# Предсказания
y_pred = xgb_model.predict(X_test)

# Визуализация
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Фактические значения')
plt.ylabel('Предсказанные значения')
plt.title('График разброса для XGBoost')
plt.show()

"""## Наивный байесовский классификатор

Наивный байесовский классификатор — это простой вероятностный классификатор, который предполагает независимость признаков.

**Пример кода**:
"""

import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

# Данные
iris = load_iris()
X = iris.data
y = iris.target

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Модель наивного байеса
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# Классификация
y_pred = gnb.predict(X_test)

# Уменьшение размерности с помощью PCA
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Визуализация
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train)
plt.title('Обучающие данные')

plt.subplot(1, 2, 2)
plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_pred)
plt.title('Тестовые данные (предсказания)')

plt.show()

"""**Объяснение кода**: Этот код демонстрирует, как использовать наивный байесовский классификатор для классификации видов ириса и визуализировать результаты с помощью уменьшения размерности данных с помощью PCA. Визуализация помогает оценить качество классификации и понять, как данные были сгруппированы.

## Алгоритм нейронных сетей

Нейронные сети — это сложные модели, которые имитируют структуру и функционирование биологических нейронов для решения задач классификации и регрессии.

**Пример кода**:
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Данные
iris = load_iris()
X = iris.data
y = iris.target

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Модель нейронной сети
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(4,)))
model.add(Dense(3, activation='softmax'))

# Компиляция модели
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Обучение модели
model.fit(X_train, y_train, epochs=10)

# Классификация
y_pred = model.predict(X_test)

"""**Объяснение кода**: В этом примере мы создаем простую нейронную сеть для классификации видов ириса. Модель состоит из двух слоев: скрытого слоя с активацией ReLU и выходного слоя с активацией softmax для многоклассовой классификации.

## Заключение

Алгоритмы машинного обучения представляют собой мощный инструмент для решения широкого спектра задач, от простой линейной регрессии до сложных нейронных сетей. Каждый алгоритм имеет свои сильные и слабые стороны, и выбор зависит от конкретной задачи и характеристик данных.
"""